
# Sampling

## 샘플링의 4가지 종류

>[!note] Sampling
**1. Simple Random Sampling**  단순 추출
**2. Systematic Sampling**  계통 추출
**3. Stratified Random Sampling**  층화 추출
**4. Cluster Sampling** 군집 추출

# 모델의 종류

> 모델의 종류는 크게 2가지로 나뉜다.

1. 방정식 기반 모델
	- 대표적으로 우리가 지금 까지 한 회귀분석, 로지스틱 회귀분석
	- 이러한 방정식 기반 모델들은 전처리의 영향을 엄청나게 크게 받는다

2. 방정식 없이 순수하게 데이터를 받아서 데이터를 가지고 학습을 하는 모델
	- 일명 Tree 기반 알고리즘 이고 잡식성 알고리즘이라고도 한다.
	- Lazy 알고리즘이라고도 하고, 전처리의 영향을 많이 받지 않기 때문에 준비가 필요없다는 점에서 "게으르다"라고 표현 하는 것임


# Decision Tree

과거에는 Linear Regression , Logistic Regression 이 마더 모델로서 중요했으나
지금은 Decision Tree 가 훨씬 중요하게 취급을 받는다.

>[!question] 왜냐하면 현존하는 (생존한) 알고리즘은 모두 Decision Tree 기반을 가지고 있기때문

◎ 그래서 시험문제에 단골로 나오고, 엔트로피 같은 것을 구하라고 자주 시킨다.

## 의사결정나무

- 지도 학습 모델
- 분류와 회귀 모두 사용 가능 (다만 구조상 , 회귀보단 분류에 더 많이 활용 된다. )
	*회귀에 사용하지 않는다는 말이 아님을 주의*
- 컴퓨터 공학에서 사용하는 Tree 자료구조를 활용
- 스무고개와 유사한 방법으로 분류 라벨을 결정

EX )
![[의사결정나무|800]]

> Decision Tree 모델은 방치시 Overfitting 에 거의 무조건 빠지게 되어있기 때문에, 적절한 Depth 조절이 필요하다
 

### Decision Tree 의 학습

- **적절한 질문을 고르는 것이 중요**
	- 스무 고개의 경우에도 좋은 질문을 먼저 던질 수록 정답을 빨리 맞춤
-  **정상/비만을 분류하는 모델을 만들기 위해서**
	- 운동 여부, 음주 여부, 식사 회수는 중요한 질문
	- 독서 여부는 중요하지 않은 질문
- **성별을 분류하는 모델을 만들기 위해서**
	- 키, 몸무게, 신발사이즈, 머리카락 길이는 중요한 질문
	- 나이, IQ, 거주 도시는 중요하지 않은 질문

1. 운동여부를 첫 번째 질문으로
2. 식사 회수와 음주여부를 두 번째 질문으로 선택

※ 좋은 질문의 예
![[의사결정나무 좋은질문]]
> 노드 내에서 <span style="color:rgb(0, 112, 192)">동질성 높음</span>
> 노드 내에서 <span style="color:rgb(0, 112, 192)">순도 높음</span>
> 노드의 <span style="color:rgb(0, 112, 192)">불순도 낮음</span>
> <span style="color:rgb(0, 112, 192)">→좋은 질문</span>


※ 나쁜 질문의 예
![[의사결정나무 나쁜질문]]

> 노드 내에서 <span style="color:rgb(255, 102, 163)">동질성 낮음</span>
> 노드 내에서 <span style="color:rgb(255, 102, 163)">순도 낮음</span>
> 노드의 <span style="color:rgb(255, 102, 163)">불순도 높음</span>
> <span style="color:rgb(255, 102, 163)">→좋지 않은 질문</span>


### 엔트로피 

>[!note] Entropy
>전자 공학에서 나온 단어가 유래이다.
>**불확실성** 또는 **무질서** 의 정도를 나타내며, **데이터의 분포가 섞여 있을 수록** (*즉, 다양한 클래스가 섞여 있을 수록* ) 엔트로피의 값이 높아지고, 한쪽으로 치우쳐 있을 수록 (*대부분이 한 클래스 일 경우)* 엔트로피 값이 낮아진다.



![[엔트로피|600]]
>수업중 농담
>- 양 극으로 갈 수록 <span style="color:rgb(0, 112, 192)">수빈</span>이처럼 순수해지고 중앙으로 올수록 <span style="color:rgb(255, 102, 163)">창성</span>이 처럼 불순해진다.

>[!note] **정보 획득(Information Gain)**:
>
>정보 이득은 분할 전후의 엔트로피 차이를 계산하여, **분할 후 엔트로피가 얼마나 줄어들었는지**를 측정하는 값
>정보 이득이 높을수록, 분할 후 데이터가 깔끔하게 분리되었다는 의미입니다.

#### **엔트로피의 역할**:

- Decision Tree는 각 분할을 할 때, **불순도**를 줄이는 방향으로 나누어야 한다.
	→ 즉, 엔트로피가 감소하는 분할을 찾는 것이 목표
- 분할 후 각 노드의 엔트로피가 낮아질수록, **그 노드는 더욱 순수**해진다. 
	→즉, 한 클래스에 가까운 노드로 분류된다는 의미.

#### 지니(Gini) 와의 비교

- **엔트로피**는 불순도를 측정할 때 로그 값을 사용하여 데이터를 얼마나 균등하게 나누었는지 평가
 
- **지니 지수**는 엔트로피와 비슷한 개념이지만, 좀 더 계산이 간단하고 빠르다. 지니 지수는 데이터가 잘못 분류될 확률을 측정한다.

지니는 외우기 쉬워서 시험에 자주나온다.

- @  지니의 계산식     **$1-p^2$**
 >[!warning] prods 에서는 information gain과 gini impurity 계산 하는 문제가 무조건 나오니까 외우고 가자.
 
 